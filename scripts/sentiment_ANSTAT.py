import pandas as pd
import numpy as np
import re
import string
from datetime import datetime
import matplotlib.pyplot as plt
import seaborn as sns
from collections import Counter
import os
import warnings
warnings.filterwarnings('ignore')

# ==============================================
# 1. CLASSE DE NETTOYAGE DE TEXTE POUR FRAN√áAIS IVOIRIEN
# ==============================================

class NettoyeurTexteIvoirien:
    """Nettoyeur sp√©cialis√© pour le fran√ßais ivoirien des commentaires Facebook"""
    
    def __init__(self):
        # Mots √† conserver (noms propres, expressions locales)
        self.mots_speciaux = {
            'ivoirien', 'ivoirienne', 'c√¥te', "c√¥te d'ivoire", 'abidjan', 'yamoussoukro',
            'bouak√©', 'daloa', 'korhogo', 'san-p√©dro', 'gagnoa',
            'ado', 'ouattara', 'alassane', 'rhdp', 'pdp', 'fpi',
            'gbagbo', 'soro', 'bedi√©', 'konan', 'gnangnan',
            'choco', '√©l√©phant', 'panth√®re',
            'ansta', 'anstats', 'anstat', 'institut', 'national', 'statistique'
        }
        
        # Expressions ivoiriennes √† conserver
        self.expressions_locales = {
            'tch√™', 'walaye', 'saya', 'g√¥h', 'faforo', 'all√¥', 'ayo',
            'wah', 'atch√™', 'atch√©', 'atch√®', 'atchi', 'atch√Æ',
            'c√© ma fa', 'c ma fa', '√ßa va aller', '√ßa va all√©',
            'w√©√©', 'w√©', 'weh', 'a√Øe', 'a√Øe a√Øe', 'a√Øe a√Øe a√Øe',
            'mon fr√®re', 'ma soeur', 'mon cher', 'ma ch√®re',
            'gros', 'grand', 'petit', 'jeune'
        }
        
        # Stop words fran√ßais
        self.stop_words_fr = {
            'le', 'la', 'les', 'de', 'du', 'des', 'un', 'une', 'et', 'est', 'en', 
            'que', 'qui', 'dans', 'pour', 'par', 'sur', 'avec', 'sans', 'sous', 
            'dont', 'o√π', 'y', '√†', 'au', 'aux', 'ce', 'cet', 'cette', 'ces',
            'mon', 'ma', 'mes', 'ton', 'ta', 'tes', 'son', 'sa', 'ses', 'notre',
            'nos', 'votre', 'vos', 'leur', 'leurs', 'on', 'nous', 'vous', 'ils',
            'elles', 'eux', 'celui', 'celle', 'ceux', 'celles', 'aucun', 'aucune',
            'certains', 'certaines', 'plusieurs', 'tout', 'tous', 'toute', 'toutes',
            'm√™me', 'comme', 'aussi', 'bien', 'tr√®s', 'plus', 'moins', 'peu',
            'beaucoup', 'trop', 'alors', 'donc', 'or', 'ni', 'car', 'mais', 'ou',
            'si', 'que', 'quand', 'comment', 'pourquoi', 'combien'
        }
        
    def nettoyer_texte(self, texte):
        """Nettoie un texte de commentaire Facebook"""
        if not isinstance(texte, str) or pd.isna(texte):
            return ""
        
        # √âtape 1: Nettoyage de base
        texte = str(texte).lower()
        
        # Supprimer les URLs
        texte = re.sub(r'http\S+|www\S+|https\S+', '', texte, flags=re.MULTILINE)
        
        # Supprimer les mentions @
        texte = re.sub(r'@\w+', '', texte)
        
        # Supprimer les hashtags
        texte = re.sub(r'#\w+', '', texte)
        
        # Garder les √©motic√¥nes de base
        emoticons = re.findall(r'[:;=][\'\"-]?[)DdpP/\\|\[\]{}@*]', texte)
        
        # Remplacer les caract√®res sp√©ciaux probl√©matiques
        texte = re.sub(r'[√¢√£√†√°√§]', 'a', texte)
        texte = re.sub(r'[√™√´√®√©]', 'e', texte)
        texte = re.sub(r'[√Æ√Ø√¨√≠]', 'i', texte)
        texte = re.sub(r'[√¥√∂√≤√≥]', 'o', texte)
        texte = re.sub(r'[√ª√º√π√∫]', 'u', texte)
        texte = re.sub(r'[√ß]', 'c', texte)
        
        # Supprimer les nombres seuls
        texte = re.sub(r'\b\d+\b', '', texte)
        
        # √âtape 2: Gestion des r√©p√©titions de caract√®res
        texte = re.sub(r'(.)\1{2,}', r'\1\1', texte)  # "bonjourrrrr" -> "bonjourr"
        
        # √âtape 3: Tokenisation et nettoyage
        tokens = re.findall(r'\b\w+\b', texte)
        
        # Filtrer les tokens
        tokens_filtres = []
        for token in tokens:
            if len(token) <= 1:
                continue
            
            # Garder les mots sp√©ciaux
            if token in self.mots_speciaux:
                tokens_filtres.append(token)
                continue
            
            # Garder les expressions locales
            if token in self.expressions_locales:
                tokens_filtres.append(token)
                continue
            
            # Supprimer les stop words
            if token in self.stop_words_fr:
                continue
            
            # V√©rifier si c'est un mot valide (au moins 2 lettres)
            if len(token) >= 2:
                tokens_filtres.append(token)
        
        # √âtape 4: Reconstruire le texte
        texte_nettoye = ' '.join(tokens_filtres)
        
        # Ajouter les √©motic√¥nes conserv√©es
        if emoticons:
            texte_nettoye += ' ' + ' '.join(emoticons)
        
        # Supprimer les espaces multiples
        texte_nettoye = re.sub(r'\s+', ' ', texte_nettoye).strip()
        
        return texte_nettoye
    
    def nettoyer_dataframe(self, df, colonne_texte):
        """Nettoie une colonne de texte dans un DataFrame"""
        print(f"üßπ Nettoyage de la colonne '{colonne_texte}'...")
        
        # Appliquer le nettoyage
        df['Text_Clean'] = df[colonne_texte].apply(self.nettoyer_texte)
        
        # Statistiques de nettoyage
        total_originaux = df[colonne_texte].dropna().shape[0]
        total_nettoyes = df['Text_Clean'][df['Text_Clean'] != ''].shape[0]
        
        print(f"   ‚úÖ Textes originaux: {total_originaux}")
        print(f"   ‚úÖ Textes nettoy√©s: {total_nettoyes}")
        print(f"   üìâ R√©duction moyenne: {((total_originaux - total_nettoyes)/total_originaux*100):.1f}%")
        
        return df

# ==============================================
# 2. ANALYSEUR DE SENTIMENTS POUR FRAN√áAIS IVOIRIEN
# ==============================================

class AnalyseurSentimentsIvoirien:
    """Analyseur de sentiments sp√©cialis√© pour le contexte ivoirien"""
    
    def __init__(self):
        self.lexique = self._creer_lexique_ivoirien()
        
    def _creer_lexique_ivoirien(self):
        """Cr√©e un lexique de sentiments adapt√© au contexte ivoirien"""
        lexique = {
            # ========== POSITIF ==========
            'bon': 0.6, 'bonne': 0.6, 'bien': 0.5, 'excellent': 0.9,
            'parfait': 0.8, 'super': 0.7, 'g√©nial': 0.7, 'formidable': 0.7,
            'f√©licitations': 0.8, 'f√©licitation': 0.8, 'f√©liciter': 0.7,
            'bravo': 0.7, 'merci': 0.4, 'remercier': 0.4,
            'content': 0.6, 'heureux': 0.7, 'satisfait': 0.6,
            'utile': 0.5, 'efficace': 0.6, 'pratique': 0.5,
            'clair': 0.4, 'pr√©cis': 0.5, 'd√©taill√©': 0.4,
            'progress': 0.5, 'progr√®s': 0.5, 'am√©lioration': 0.5,
            'm√©canique': 0.3,  # positif dans contexte ivoirien
            'choco': 0.4,  # supporters positifs
            
            # ========== N√âGATIF ==========
            'mauvais': -0.6, 'mal': -0.5, 'nul': -0.7, 'nulle': -0.7,
            'pire': -0.8, 'horrible': -0.8, 'terrible': -0.7,
            'probl√®me': -0.4, 'difficult√©': -0.4, 'erreur': -0.5,
            'faux': -0.6, 'incorrect': -0.5, 'inexact': -0.5,
            'incompr√©hensible': -0.4, 'confus': -0.4, 'compliqu√©': -0.3,
            'cher': -0.3, 'co√ªteux': -0.4, 'trop': -0.2,
            'manque': -0.3, 'absent': -0.4, 'insuffisant': -0.4,
            'tch√™': -0.5,  # expression n√©gative ivoirienne
            'saya': -0.6,  # tr√®s n√©gatif
            
            # ========== CONTEXTE STATISTIQUE ==========
            'statistique': 0.0, 'donn√©e': 0.0, 'chiffre': 0.0,
            '√©tude': 0.0, 'recherche': 0.1, 'analyse': 0.0,
            'enqu√™te': 0.0, 'sondage': 0.0, 'r√©sultat': 0.0,
            'rapport': 0.0, 'publication': 0.0, 'information': 0.1,
            'anstat': 0.0, 'institut': 0.0, 'national': 0.0,
            
            # ========== NEUTRE/ADMINISTRATIF ==========
            'question': 0.0, 'demande': 0.0, 'r√©ponse': 0.0,
            'explication': 0.0, 'd√©tail': 0.0, 'exemple': 0.0,
            'minist√®re': 0.0, 'gouvernement': 0.0, 'administration': 0.0,
            'service': 0.0, 'public': 0.0, 'citoyen': 0.0,
            'population': 0.0, 'habitant': 0.0, 'r√©sident': 0.0,
        }
        return lexique
    
    def analyser_sentiment_texte(self, texte):
        """Analyse le sentiment d'un texte"""
        if not texte or not isinstance(texte, str):
            return {'score': 0, 'sentiment': 'NEUTRE', 'confiance': 0}
        
        # Tokenisation
        mots = re.findall(r'\b\w+\b', texte.lower())
        
        if not mots:
            return {'score': 0, 'sentiment': 'NEUTRE', 'confiance': 0}
        
        # Calcul du score
        scores = []
        negation = False
        
        for mot in mots:
            if mot in ['pas', 'non', 'jamais', 'rien', 'aucun']:
                negation = True
                continue
            
            if mot in self.lexique:
                score = self.lexique[mot]
                if negation:
                    score = -score * 0.7
                    negation = False
                scores.append(score)
            else:
                negation = False
        
        if scores:
            score_moyen = np.mean(scores)
            confiance = min(len(scores) / 10, 1.0)
        else:
            score_moyen = 0
            confiance = 0
        
        # D√©terminer la cat√©gorie
        if score_moyen > 0.1:
            sentiment = 'POSITIF'
        elif score_moyen < -0.1:
            sentiment = 'N√âGATIF'
        else:
            sentiment = 'NEUTRE'
        
        return {
            'score': round(score_moyen, 3),
            'sentiment': sentiment,
            'confiance': round(confiance, 2),
            'mots_analys√©s': len(scores)
        }
    
    def analyser_dataframe(self, df, colonne_texte='Text_Clean'):
        """Analyse les sentiments d'une colonne de DataFrame"""
        print(f"üîç Analyse des sentiments de '{colonne_texte}'...")
        
        resultats = []
        
        for idx, texte in enumerate(df[colonne_texte]):
            if pd.isna(texte) or texte == '':
                resultats.append({'score': 0, 'sentiment': 'NEUTRE', 'confiance': 0})
            else:
                resultats.append(self.analyser_sentiment_texte(str(texte)))
            
            # Afficher la progression
            if (idx + 1) % 50 == 0:
                print(f"   ‚úì {idx + 1}/{len(df)} textes analys√©s")
        
        # Cr√©er un DataFrame de r√©sultats
        df_resultats = pd.DataFrame(resultats)
        
        # Fusionner avec le DataFrame original
        df['score_sentiment'] = df_resultats['score']
        df['sentiment'] = df_resultats['sentiment']
        df['confiance_sentiment'] = df_resultats['confiance']
        
        print(f"‚úÖ Analyse termin√©e: {len(df)} commentaires trait√©s")
        
        return df

# ==============================================
# 3. G√âN√âRATEUR DE RAPPORT
# ==============================================

class GenerateurRapportANSTAT:
    """G√©n√©rateur de rapport d'analyse pour ANSTAT"""
    
    def __init__(self, df):
        self.df = df.copy()
        self.resultats = {}
        
    def analyser_distribution(self):
        """Analyse la distribution des sentiments"""
        if 'sentiment' not in self.df.columns:
            raise ValueError("La colonne 'sentiment' n'existe pas dans le DataFrame")
        
        distribution = self.df['sentiment'].value_counts()
        pourcentages = (distribution / len(self.df) * 100).round(1)
        
        self.resultats['distribution'] = {
            'counts': distribution.to_dict(),
            'percentages': pourcentages.to_dict()
        }
        
        return self
    
    def analyser_tendance_temporelle(self):
        """Analyse l'√©volution des sentiments dans le temps"""
        if 'Post Date-Time' not in self.df.columns:
            print("‚ö†Ô∏è  Colonne 'Post Date-Time' non trouv√©e, analyse temporelle ignor√©e")
            return self
        
        # Convertir en datetime
        self.df['date'] = pd.to_datetime(self.df['Post Date-Time']).dt.date
        
        # Grouper par jour
        daily_stats = self.df.groupby('date').agg({
            'score_sentiment': 'mean',
            'sentiment': lambda x: (x == 'POSITIF').sum() / len(x) * 100
        }).rename(columns={'sentiment': 'pct_positif'})
        
        self.resultats['tendance_temporelle'] = daily_stats
        
        return self
    
    def analyser_mots_cles(self, top_n=20):
        """Analyse les mots-cl√©s les plus fr√©quents"""
        if 'Text_Clean' not in self.df.columns:
            print("‚ö†Ô∏è  Colonne 'Text_Clean' non trouv√©e")
            return self
        
        # Concat√©ner tous les textes
        all_text = ' '.join(self.df['Text_Clean'].dropna().astype(str).tolist())
        
        # Extraire les mots
        words = re.findall(r'\b[a-z√©√®√™√´√†√¢√§√¥√∂√ª√º√ß]{3,}\b', all_text.lower())
        
        # Compter les occurrences
        word_counts = Counter(words)
        
        # Filtrer les mots communs non informatifs
        common_words = {'que', 'est', 'pas', 'pour', 'dans', 'avec', 'mais', 'son', 'ses',
                       'une', 'des', 'les', 'aux', 'du', 'de', 'la', 'le', 'et', 'ou'}
        
        filtered_counts = {word: count for word, count in word_counts.items() 
                          if word not in common_words}
        
        # Top N mots
        top_words = dict(sorted(filtered_counts.items(), 
                               key=lambda x: x[1], reverse=True)[:top_n])
        
        self.resultats['mots_cles'] = top_words
        
        return self
    
    def analyser_longueur_textes(self):
        """Analyse la longueur des textes"""
        if 'Text_Clean' not in self.df.columns:
            return self
        
        self.df['longueur_texte'] = self.df['Text_Clean'].apply(
            lambda x: len(str(x).split()) if pd.notna(x) else 0
        )
        
        stats = {
            'moyenne': self.df['longueur_texte'].mean(),
            'mediane': self.df['longueur_texte'].median(),
            'max': self.df['longueur_texte'].max(),
            'min': self.df['longueur_texte'].min()
        }
        
        self.resultats['longueur_textes'] = stats
        
        return self
    
    def generer_rapport_texte(self, chemin_sortie=None):
        """G√©n√®re un rapport texte d√©taill√©"""
        rapport = []
        
        # En-t√™te
        rapport.append("="*80)
        rapport.append("RAPPORT D'ANALYSE DES COMMENTAIRES - ANSTAT C√îTE D'IVOIRE")
        rapport.append("="*80)
        rapport.append("")
        
        # 1. Synth√®se
        rapport.append("1. SYNTH√àSE DE L'ANALYSE")
        rapport.append("-"*40)
        rapport.append(f"P√©riode d'analyse : {self.df['date'].min() if 'date' in self.df.columns else 'N/A'} "
                      f"au {self.df['date'].max() if 'date' in self.df.columns else 'N/A'}")
        rapport.append(f"Nombre total de commentaires : {len(self.df)}")
        rapport.append("")
        
        # 2. Distribution des sentiments
        rapport.append("2. DISTRIBUTION DES SENTIMENTS")
        rapport.append("-"*40)
        
        if 'distribution' in self.resultats:
            for sentiment, count in self.resultats['distribution']['counts'].items():
                pct = self.resultats['distribution']['percentages'].get(sentiment, 0)
                rapport.append(f"  ‚Ä¢ {sentiment:10} : {count:4d} commentaires ({pct:5.1f}%)")
        rapport.append("")
        
        # 3. Scores moyens
        rapport.append("3. SCORES MOYENS")
        rapport.append("-"*40)
        if 'score_sentiment' in self.df.columns:
            rapport.append(f"  Score moyen global : {self.df['score_sentiment'].mean():.3f}")
            rapport.append(f"  Score m√©dian : {self.df['score_sentiment'].median():.3f}")
            
            # Scores par cat√©gorie
            for sentiment in self.df['sentiment'].unique():
                score_moyen = self.df[self.df['sentiment'] == sentiment]['score_sentiment'].mean()
                rapport.append(f"  ‚Ä¢ {sentiment} : {score_moyen:.3f}")
        rapport.append("")
        
        # 4. Mots-cl√©s
        rapport.append("4. TH√âMATIQUES PRINCIPALES")
        rapport.append("-"*40)
        
        if 'mots_cles' in self.resultats:
            rapport.append("  Mots les plus fr√©quents :")
            for i, (mot, freq) in enumerate(self.resultats['mots_cles'].items(), 1):
                rapport.append(f"    {i:2d}. {mot:15} : {freq:3d} mentions")
                if i >= 10:  # Limiter √† 10 mots
                    break
        rapport.append("")
        
        # 5. Analyse temporelle
        if 'tendance_temporelle' in self.resultats:
            rapport.append("5. √âVOLUTION TEMPORELLE")
            rapport.append("-"*40)
            
            daily = self.resultats['tendance_temporelle']
            rapport.append(f"  Score moyen quotidien : {daily['score_sentiment'].mean():.3f}")
            rapport.append(f"  % moyen de positivit√© : {daily['pct_positif'].mean():.1f}%")
            rapport.append("")
        
        # 6. Exemples significatifs
        rapport.append("6. EXEMPLES DE COMMENTAIRES")
        rapport.append("-"*40)
        
        # Exemples positifs
        positifs = self.df[self.df['sentiment'] == 'POSITIF'].sort_values('score_sentiment', ascending=False).head(3)
        if len(positifs) > 0:
            rapport.append("  Commentaires positifs :")
            for idx, row in positifs.iterrows():
                texte = str(row['Text_Clean'])[:80] + "..." if len(str(row['Text_Clean'])) > 80 else str(row['Text_Clean'])
                rapport.append(f"    ‚úì Score: {row['score_sentiment']:.3f} - \"{texte}\"")
        
        # Exemples n√©gatifs
        negatifs = self.df[self.df['sentiment'] == 'N√âGATIF'].sort_values('score_sentiment').head(2)
        if len(negatifs) > 0:
            rapport.append("\n  Commentaires n√©gatifs :")
            for idx, row in negatifs.iterrows():
                texte = str(row['Text_Clean'])[:80] + "..." if len(str(row['Text_Clean'])) > 80 else str(row['Text_Clean'])
                rapport.append(f"    ‚úó Score: {row['score_sentiment']:.3f} - \"{texte}\"")
        rapport.append("")
        
        # 7. Recommandations
        rapport.append("7. RECOMMANDATIONS")
        rapport.append("-"*40)
        
        if 'distribution' in self.resultats:
            pct_positif = self.resultats['distribution']['percentages'].get('POSITIF', 0)
            pct_negatif = self.resultats['distribution']['percentages'].get('N√âGATIF', 0)
            
            if pct_positif > 30:
                rapport.append("  ‚úÖ Engagement tr√®s positif")
                rapport.append("     ‚Üí Capitaliser sur cette dynamique en mettant en avant")
                rapport.append("       les retours positifs")
            elif pct_negatif > 20:
                rapport.append("  ‚ö†Ô∏è  Niveau de critique √©lev√©")
                rapport.append("     ‚Üí Analyser en d√©tail les pr√©occupations")
                rapport.append("     ‚Üí R√©pondre syst√©matiquement aux commentaires")
            else:
                rapport.append("  ‚öñÔ∏è  Situation √©quilibr√©e")
                rapport.append("     ‚Üí Maintenir le dialogue avec les citoyens")
                rapport.append("     ‚Üí Renforcer la communication sur les donn√©es")
        
        rapport.append("")
        
        # 8. Conclusion
        rapport.append("8. CONCLUSION")
        rapport.append("-"*40)
        rapport.append("  Cette analyse r√©v√®le les perceptions des citoyens vis-√†-vis")
        rapport.append("  des publications de l'ANSTAT. Les r√©sultats peuvent servir")
        rapport.append("  √† am√©liorer la communication et l'engagement citoyen.")
        rapport.append("")
        
        # Pied de page
        rapport.append("="*80)
        rapport.append(f"G√©n√©r√© le : {datetime.now().strftime('%d/%m/%Y %H:%M')}")
        rapport.append("M√©thodologie : Nettoyage de texte + Analyse lexicale adapt√©e")
        rapport.append("="*80)
        
        # Convertir en texte
        rapport_texte = '\n'.join(rapport)
        
        # Sauvegarder
        if chemin_sortie:
            with open(chemin_sortie, 'w', encoding='utf-8') as f:
                f.write(rapport_texte)
            print(f"‚úÖ Rapport sauvegard√© : {chemin_sortie}")
        else:
            # Sauvegarde par d√©faut
            dossier_rapports = 'rapports_anstat'
            os.makedirs(dossier_rapports, exist_ok=True)
            
            nom_fichier = f"rapport_anstat_{datetime.now().strftime('%Y%m%d_%H%M')}.txt"
            chemin_sauvegarde = os.path.join(dossier_rapports, nom_fichier)
            
            with open(chemin_sauvegarde, 'w', encoding='utf-8') as f:
                f.write(rapport_texte)
            print(f"‚úÖ Rapport sauvegard√© : {chemin_sauvegarde}")
        
        return rapport_texte
    
    def generer_graphiques(self, dossier_output="graphiques_anstat"):
        """G√©n√®re des graphiques d'analyse"""
        os.makedirs(dossier_output, exist_ok=True)
        
        # 1. Camembert des sentiments
        plt.figure(figsize=(10, 8))
        if 'distribution' in self.resultats:
            labels = list(self.resultats['distribution']['counts'].keys())
            sizes = list(self.resultats['distribution']['counts'].values())
            colors = ['#4CAF50', '#F44336', '#FFC107'][:len(labels)]
            
            plt.pie(sizes, labels=labels, colors=colors, autopct='%1.1f%%', startangle=90)
            plt.title('Distribution des Sentiments - ANSTAT', fontsize=14, fontweight='bold')
            plt.axis('equal')
            plt.savefig(f'{dossier_output}/distribution_sentiments.png', dpi=150, bbox_inches='tight')
            plt.close()
        
        # 2. Histogramme des scores
        plt.figure(figsize=(12, 6))
        plt.hist(self.df['score_sentiment'], bins=30, color='#2196F3', edgecolor='black', alpha=0.7)
        plt.axvline(x=0, color='red', linestyle='--', linewidth=2, label='Neutre')
        plt.xlabel('Score de Sentiment')
        plt.ylabel('Nombre de Commentaires')
        plt.title('Distribution des Scores de Sentiment', fontsize=14, fontweight='bold')
        plt.legend()
        plt.grid(True, alpha=0.3)
        plt.savefig(f'{dossier_output}/histogramme_scores.png', dpi=150, bbox_inches='tight')
        plt.close()
        
        # 3. Top mots
        if 'mots_cles' in self.resultats:
            plt.figure(figsize=(12, 6))
            mots = list(self.resultats['mots_cles'].keys())[:15]
            frequences = list(self.resultats['mots_cles'].values())[:15]
            
            plt.barh(range(len(mots)), frequences, color='#FF9800')
            plt.yticks(range(len(mots)), mots)
            plt.xlabel('Fr√©quence')
            plt.title('Top 15 des Mots les Plus Fr√©quents', fontsize=14, fontweight='bold')
            plt.gca().invert_yaxis()
            plt.tight_layout()
            plt.savefig(f'{dossier_output}/top_mots.png', dpi=150, bbox_inches='tight')
            plt.close()
        
        print(f"‚úÖ Graphiques sauvegard√©s dans '{dossier_output}'")
        
        return dossier_output

# ==============================================
# 4. PIPELINE COMPL√àTE D'ANALYSE
# ==============================================

def analyser_fichier_anstat(chemin_fichier):
    """Pipeline compl√®te d'analyse du fichier ANSTAT"""
    
    print("üöÄ D√âMARRAGE DE L'ANALYSE ANSTAT")
    print("="*60)
    
    try:
        # 1. Charger le fichier
        print(f"\nüìÇ Chargement du fichier: {chemin_fichier}")
        df = pd.read_excel(chemin_fichier)
        print(f"‚úÖ {len(df)} commentaires charg√©s")
        print(f"   Colonnes disponibles: {list(df.columns)}")
        
        # 2. Nettoyer les textes
        print("\nüßπ PHASE 1: NETTOYAGE DES TEXTES")
        nettoyeur = NettoyeurTexteIvoirien()
        
        # Identifier la colonne de texte
        if 'Comment Text' in df.columns:
            colonne_texte = 'Comment Text'
        elif 'texte_original' in df.columns:
            colonne_texte = 'texte_original'
        else:
            # Chercher une colonne contenant du texte
            text_columns = [col for col in df.columns if 'text' in col.lower() or 'comment' in col.lower()]
            if text_columns:
                colonne_texte = text_columns[0]
            else:
                colonne_texte = df.columns[1]  # Deuxi√®me colonne par d√©faut
        
        print(f"   Colonne de texte identifi√©e: '{colonne_texte}'")
        
        # Nettoyer
        df = nettoyeur.nettoyer_dataframe(df, colonne_texte)
        
        # 3. Analyser les sentiments
        print("\nüîç PHASE 2: ANALYSE DES SENTIMENTS")
        analyseur = AnalyseurSentimentsIvoirien()
        df = analyseur.analyser_dataframe(df, 'Text_Clean')
        
        # 4. G√©n√©rer le rapport
        print("\nüìä PHASE 3: G√âN√âRATION DU RAPPORT")
        generateur = GenerateurRapportANSTAT(df)
        
        # Ex√©cuter les analyses
        generateur.analyser_distribution()
        generateur.analyser_tendance_temporelle()
        generateur.analyser_mots_cles()
        generateur.analyser_longueur_textes()
        
        # G√©n√©rer le rapport texte
        rapport = generateur.generer_rapport_texte()
        
        # G√©n√©rer les graphiques
        generateur.generer_graphiques()
        
        # 5. Sauvegarder les r√©sultats
        print("\nüíæ PHASE 4: SAUVEGARDE DES R√âSULTATS")
        
        # Cr√©er le dossier de sortie
        dossier_resultats = 'resultats_anstat'
        os.makedirs(dossier_resultats, exist_ok=True)
        
        # Sauvegarder le DataFrame avec les r√©sultats
        nom_fichier_sortie = os.path.basename(chemin_fichier).replace('.xlsx', '_analyse_complete.xlsx')
        chemin_sortie = os.path.join(dossier_resultats, nom_fichier_sortie)
        
        df.to_excel(chemin_sortie, index=False)
        print(f"‚úÖ Donn√©es analys√©es sauvegard√©es: {chemin_sortie}")
        
        # Statistiques finales
        print("\n" + "="*60)
        print("üìà STATISTIQUES FINALES")
        print("="*60)
        
        distribution = generateur.resultats.get('distribution', {})
        if 'counts' in distribution:
            for sentiment, count in distribution['counts'].items():
                pct = distribution['percentages'].get(sentiment, 0)
                print(f"   {sentiment:10}: {count:4d} ({pct:5.1f}%)")
        
        if 'score_sentiment' in df.columns:
            print(f"\n   Score moyen global: {df['score_sentiment'].mean():.3f}")
            print(f"   Score m√©dian: {df['score_sentiment'].median():.3f}")
        
        # Afficher un extrait du rapport
        print("\n" + "="*60)
        print("üìã EXTRAIT DU RAPPORT")
        print("="*60)
        lignes_rapport = rapport.split('\n')
        for ligne in lignes_rapport[:20]:  # Afficher les 20 premi√®res lignes
            print(ligne)
        
        print("\n" + "="*60)
        print("‚úÖ ANALYSE COMPL√âT√âE AVEC SUCC√àS")
        print("="*60)
        
        return df
        
    except Exception as e:
        print(f"\n‚ùå ERREUR: {str(e)}")
        import traceback
        traceback.print_exc()
        return None

# ==============================================
# 5. EX√âCUTION PRINCIPALE
# ==============================================

if __name__ == "__main__":
    # Chemin du fichier ANSTAT
    chemin_fichier = r"C:\Users\Sy Savane Idriss\project_sentiment_fb\data\Commentaire_ANSTAT_4_semaine.xlsx"
    
    # V√©rifier si le fichier existe
    if not os.path.exists(chemin_fichier):
        print(f"‚ùå Fichier introuvable: {chemin_fichier}")
        print("V√©rifiez le chemin et r√©essayez.")
    else:
        # Ex√©cuter l'analyse compl√®te
        resultats = analyser_fichier_anstat(chemin_fichier)
        
        if resultats is not None:
            print("\nüìÅ FICHIERS G√âN√âR√âS :")
            print("   ‚Ä¢ resultats_anstat/ : Donn√©es analys√©es")
            print("   ‚Ä¢ rapports_anstat/ : Rapport texte")
            print("   ‚Ä¢ graphiques_anstat/ : Graphiques d'analyse")
            
            print("\nüéØ PROCHAINES √âTAPES :")
            print("   1. Consulter le rapport texte complet")
            print("   2. Examiner les graphiques g√©n√©r√©s")
            print("   3. Analyser les mots-cl√©s identifi√©s")
            print("   4. Adapter la strat√©gie de communication")